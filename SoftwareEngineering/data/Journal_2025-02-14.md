I've been reading about generative AI lately.  I found the best, most concise
explanation of RAG in
[Ken Kousen's blog](https://kenkousen.substack.com/p/tales-from-the-jar-side-ai-vs-rap).

> **Prompt stuffing** is where you download the information the model needs and
> send it to the model along with your request.  That’s a problem if the
> information you’re sending is bigger than the model can handle in a single
> request.  **RAG** is short for **Retrieval Augmented Generation**, and is a
> way of splitting the information into chunks, saving the encoded version of
> each into a vector database, and then, when a query comes, figuring out which
> chunks are the most relevant and send only those to the model.

It doesn't have any details, but it draws a clear picture of what is happening
and why.

There is a very nice reference definition from Bharani Subramaniam on
[Martin Fowler's blog](https://martinfowler.com/articles/gen-ai-patterns/), but
it is fairly high-level.  The article puts it in context with the other GenAI
techniques, especially when you use it in hybrid searches.

For an example with code, I really liked
[this article](https://dev.to/mongodb/how-to-make-a-rag-application-with-langchain4j-1mad)
by Tim Kelly that implements a RAG example using LangChain4j and MongoDB Atlas.
I wish there was an easy way to use a local MongoDB instance, running in a
Docker container, to do vector search.  But it seems we have to use a cloud
provider like MongoDB Atlas.

I tried to run a small DeepSeek model in a local Docker container on my laptop.
It worked, but it was very slow.  I can understand why you might want to use a
cloud service to do vector search.  They need pretty heavy hardware to give
reasonable performance.  Here is a short post on the
[OpenAI community blog](https://community.openai.com/t/mongodb-as-both-a-vector-and-numeric-search-database/595905)
that talks about it a little.
